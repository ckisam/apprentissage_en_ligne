{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet d'apprentissage en ligne\n",
    "\n",
    "Ce projet s'appuie sur un article de Elad Hazan, Adam Kalai, Satyen Kale et Amit Agarwal « Logarithmic Regret Algorithms for Online Convex Optimization ». Les auteurs y présentent trois algorithmes permettant de faire de l'optimisation convexe en ligne, en garantissant sous certaines conditions un regret logarithmique à la date $T$. Avec un focntion de perte convexe $f_t$ et une décision $x_t$ à la date $t$, le regret est défini par:\n",
    "$$ Regret : R_T = \\sum\\limits_{t=1}^T f_t(x_t) - \\underset{x \\in K}{\\text{min}} \\sum\\limits_{t=1}^T f_t(x)$$\n",
    "\n",
    "\n",
    "Les trois algorithmes étudiés dans cet article sont :\n",
    "- **Online Gradient Descent**\n",
    "- **Online Newton Step**\n",
    "- **Exponentially Weighted Online Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'jyquickhelper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-27a1762e8b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mjyquickhelper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madd_notebook_menu\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0madd_notebook_menu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'jyquickhelper'"
     ]
    }
   ],
   "source": [
    "from jyquickhelper import add_notebook_menu\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données\n",
    "\n",
    "On exploite un jeu de données de sélection de portefeuille. Il s'agit de l'indice boursier S&P 500 pour 490 sociétés, durant 1000 jours de marché de la période 2001 à 2005. Le jeu de données a été obtenu à ce [lien](http://ocobook.cs.princeton.edu/links.htm) au format Matlab puis converti au format CSV.\n",
    "\n",
    "On cherche donc, au cours du temps, à obtenir le meilleur portefeuille parmi les 490 actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du DF 'data' : (1000, 490)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABI</th>\n",
       "      <th>ABK</th>\n",
       "      <th>ABS</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACE</th>\n",
       "      <th>ACS</th>\n",
       "      <th>A</th>\n",
       "      <th>...</th>\n",
       "      <th>XEL</th>\n",
       "      <th>XL</th>\n",
       "      <th>XLNX</th>\n",
       "      <th>XOM</th>\n",
       "      <th>XRX</th>\n",
       "      <th>XTO</th>\n",
       "      <th>YHOO</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZMH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.41</td>\n",
       "      <td>67.82</td>\n",
       "      <td>80.35</td>\n",
       "      <td>27.58</td>\n",
       "      <td>76.69</td>\n",
       "      <td>23.50</td>\n",
       "      <td>37.71</td>\n",
       "      <td>55.50</td>\n",
       "      <td>55.78</td>\n",
       "      <td>35.66</td>\n",
       "      <td>...</td>\n",
       "      <td>18.51</td>\n",
       "      <td>66.38</td>\n",
       "      <td>26.44</td>\n",
       "      <td>58.03</td>\n",
       "      <td>14.20</td>\n",
       "      <td>40.69</td>\n",
       "      <td>40.23</td>\n",
       "      <td>48.79</td>\n",
       "      <td>75.63</td>\n",
       "      <td>62.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.87</td>\n",
       "      <td>68.10</td>\n",
       "      <td>80.50</td>\n",
       "      <td>27.35</td>\n",
       "      <td>77.00</td>\n",
       "      <td>23.85</td>\n",
       "      <td>38.16</td>\n",
       "      <td>55.86</td>\n",
       "      <td>55.71</td>\n",
       "      <td>35.49</td>\n",
       "      <td>...</td>\n",
       "      <td>18.85</td>\n",
       "      <td>67.57</td>\n",
       "      <td>25.93</td>\n",
       "      <td>58.34</td>\n",
       "      <td>14.17</td>\n",
       "      <td>40.99</td>\n",
       "      <td>40.19</td>\n",
       "      <td>49.10</td>\n",
       "      <td>76.53</td>\n",
       "      <td>62.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.60</td>\n",
       "      <td>69.66</td>\n",
       "      <td>79.31</td>\n",
       "      <td>27.13</td>\n",
       "      <td>76.15</td>\n",
       "      <td>23.77</td>\n",
       "      <td>38.95</td>\n",
       "      <td>55.39</td>\n",
       "      <td>55.08</td>\n",
       "      <td>35.65</td>\n",
       "      <td>...</td>\n",
       "      <td>18.91</td>\n",
       "      <td>66.57</td>\n",
       "      <td>25.94</td>\n",
       "      <td>58.74</td>\n",
       "      <td>13.99</td>\n",
       "      <td>40.80</td>\n",
       "      <td>41.11</td>\n",
       "      <td>49.10</td>\n",
       "      <td>76.41</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.33</td>\n",
       "      <td>69.34</td>\n",
       "      <td>79.39</td>\n",
       "      <td>27.18</td>\n",
       "      <td>76.49</td>\n",
       "      <td>23.99</td>\n",
       "      <td>38.98</td>\n",
       "      <td>56.05</td>\n",
       "      <td>55.48</td>\n",
       "      <td>35.69</td>\n",
       "      <td>...</td>\n",
       "      <td>18.89</td>\n",
       "      <td>67.42</td>\n",
       "      <td>26.46</td>\n",
       "      <td>60.11</td>\n",
       "      <td>14.16</td>\n",
       "      <td>42.64</td>\n",
       "      <td>42.13</td>\n",
       "      <td>49.05</td>\n",
       "      <td>76.66</td>\n",
       "      <td>61.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.89</td>\n",
       "      <td>67.11</td>\n",
       "      <td>79.28</td>\n",
       "      <td>27.49</td>\n",
       "      <td>77.54</td>\n",
       "      <td>24.42</td>\n",
       "      <td>38.99</td>\n",
       "      <td>56.28</td>\n",
       "      <td>55.40</td>\n",
       "      <td>35.65</td>\n",
       "      <td>...</td>\n",
       "      <td>18.79</td>\n",
       "      <td>73.74</td>\n",
       "      <td>26.36</td>\n",
       "      <td>59.87</td>\n",
       "      <td>14.19</td>\n",
       "      <td>42.35</td>\n",
       "      <td>42.50</td>\n",
       "      <td>49.60</td>\n",
       "      <td>76.23</td>\n",
       "      <td>62.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 490 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AA   AAPL    ABC    ABI    ABK    ABS    ABT    ACE    ACS      A  \\\n",
       "0  27.41  67.82  80.35  27.58  76.69  23.50  37.71  55.50  55.78  35.66   \n",
       "1  27.87  68.10  80.50  27.35  77.00  23.85  38.16  55.86  55.71  35.49   \n",
       "2  27.60  69.66  79.31  27.13  76.15  23.77  38.95  55.39  55.08  35.65   \n",
       "3  27.33  69.34  79.39  27.18  76.49  23.99  38.98  56.05  55.48  35.69   \n",
       "4  26.89  67.11  79.28  27.49  77.54  24.42  38.99  56.28  55.40  35.65   \n",
       "\n",
       "   ...      XEL     XL   XLNX    XOM    XRX    XTO   YHOO    YUM   ZION    ZMH  \n",
       "0  ...    18.51  66.38  26.44  58.03  14.20  40.69  40.23  48.79  75.63  62.67  \n",
       "1  ...    18.85  67.57  25.93  58.34  14.17  40.99  40.19  49.10  76.53  62.50  \n",
       "2  ...    18.91  66.57  25.94  58.74  13.99  40.80  41.11  49.10  76.41  61.89  \n",
       "3  ...    18.89  67.42  26.46  60.11  14.16  42.64  42.13  49.05  76.66  61.50  \n",
       "4  ...    18.79  73.74  26.36  59.87  14.19  42.35  42.50  49.60  76.23  62.12  \n",
       "\n",
       "[5 rows x 490 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "print(\"Dimensions du DF 'data' : {}\".format(data.shape))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A completer le traitement effectuer sur les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du DF 'prixFull' : (999, 490)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABI</th>\n",
       "      <th>ABK</th>\n",
       "      <th>ABS</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACE</th>\n",
       "      <th>ACS</th>\n",
       "      <th>A</th>\n",
       "      <th>...</th>\n",
       "      <th>XEL</th>\n",
       "      <th>XL</th>\n",
       "      <th>XLNX</th>\n",
       "      <th>XOM</th>\n",
       "      <th>XRX</th>\n",
       "      <th>XTO</th>\n",
       "      <th>YHOO</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZMH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.016782</td>\n",
       "      <td>1.004129</td>\n",
       "      <td>1.001867</td>\n",
       "      <td>0.991661</td>\n",
       "      <td>1.004042</td>\n",
       "      <td>1.014894</td>\n",
       "      <td>1.011933</td>\n",
       "      <td>1.006486</td>\n",
       "      <td>0.998745</td>\n",
       "      <td>0.995233</td>\n",
       "      <td>...</td>\n",
       "      <td>1.018368</td>\n",
       "      <td>1.017927</td>\n",
       "      <td>0.980711</td>\n",
       "      <td>1.005342</td>\n",
       "      <td>0.997887</td>\n",
       "      <td>1.007373</td>\n",
       "      <td>0.999006</td>\n",
       "      <td>1.006354</td>\n",
       "      <td>1.011900</td>\n",
       "      <td>0.997287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.990312</td>\n",
       "      <td>1.022907</td>\n",
       "      <td>0.985217</td>\n",
       "      <td>0.991956</td>\n",
       "      <td>0.988961</td>\n",
       "      <td>0.996646</td>\n",
       "      <td>1.020702</td>\n",
       "      <td>0.991586</td>\n",
       "      <td>0.988691</td>\n",
       "      <td>1.004508</td>\n",
       "      <td>...</td>\n",
       "      <td>1.003183</td>\n",
       "      <td>0.985201</td>\n",
       "      <td>1.000386</td>\n",
       "      <td>1.006856</td>\n",
       "      <td>0.987297</td>\n",
       "      <td>0.995365</td>\n",
       "      <td>1.022891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998432</td>\n",
       "      <td>0.990240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.990217</td>\n",
       "      <td>0.995406</td>\n",
       "      <td>1.001009</td>\n",
       "      <td>1.001843</td>\n",
       "      <td>1.004465</td>\n",
       "      <td>1.009255</td>\n",
       "      <td>1.000770</td>\n",
       "      <td>1.011916</td>\n",
       "      <td>1.007262</td>\n",
       "      <td>1.001122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998942</td>\n",
       "      <td>1.012769</td>\n",
       "      <td>1.020046</td>\n",
       "      <td>1.023323</td>\n",
       "      <td>1.012152</td>\n",
       "      <td>1.045098</td>\n",
       "      <td>1.024811</td>\n",
       "      <td>0.998982</td>\n",
       "      <td>1.003272</td>\n",
       "      <td>0.993698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.983900</td>\n",
       "      <td>0.967840</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>1.011405</td>\n",
       "      <td>1.013727</td>\n",
       "      <td>1.017924</td>\n",
       "      <td>1.000257</td>\n",
       "      <td>1.004103</td>\n",
       "      <td>0.998558</td>\n",
       "      <td>0.998879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994706</td>\n",
       "      <td>1.093741</td>\n",
       "      <td>0.996221</td>\n",
       "      <td>0.996007</td>\n",
       "      <td>1.002119</td>\n",
       "      <td>0.993199</td>\n",
       "      <td>1.008782</td>\n",
       "      <td>1.011213</td>\n",
       "      <td>0.994391</td>\n",
       "      <td>1.010081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.992934</td>\n",
       "      <td>0.991208</td>\n",
       "      <td>0.999622</td>\n",
       "      <td>0.994907</td>\n",
       "      <td>0.985040</td>\n",
       "      <td>1.007371</td>\n",
       "      <td>1.008464</td>\n",
       "      <td>0.980455</td>\n",
       "      <td>1.006318</td>\n",
       "      <td>0.997756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984566</td>\n",
       "      <td>0.979116</td>\n",
       "      <td>1.003035</td>\n",
       "      <td>0.996492</td>\n",
       "      <td>0.991543</td>\n",
       "      <td>1.016057</td>\n",
       "      <td>0.996706</td>\n",
       "      <td>0.991532</td>\n",
       "      <td>0.997770</td>\n",
       "      <td>1.004185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 490 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AA      AAPL       ABC       ABI       ABK       ABS       ABT  \\\n",
       "0  1.016782  1.004129  1.001867  0.991661  1.004042  1.014894  1.011933   \n",
       "1  0.990312  1.022907  0.985217  0.991956  0.988961  0.996646  1.020702   \n",
       "2  0.990217  0.995406  1.001009  1.001843  1.004465  1.009255  1.000770   \n",
       "3  0.983900  0.967840  0.998614  1.011405  1.013727  1.017924  1.000257   \n",
       "4  0.992934  0.991208  0.999622  0.994907  0.985040  1.007371  1.008464   \n",
       "\n",
       "        ACE       ACS         A    ...          XEL        XL      XLNX  \\\n",
       "0  1.006486  0.998745  0.995233    ...     1.018368  1.017927  0.980711   \n",
       "1  0.991586  0.988691  1.004508    ...     1.003183  0.985201  1.000386   \n",
       "2  1.011916  1.007262  1.001122    ...     0.998942  1.012769  1.020046   \n",
       "3  1.004103  0.998558  0.998879    ...     0.994706  1.093741  0.996221   \n",
       "4  0.980455  1.006318  0.997756    ...     0.984566  0.979116  1.003035   \n",
       "\n",
       "        XOM       XRX       XTO      YHOO       YUM      ZION       ZMH  \n",
       "0  1.005342  0.997887  1.007373  0.999006  1.006354  1.011900  0.997287  \n",
       "1  1.006856  0.987297  0.995365  1.022891  1.000000  0.998432  0.990240  \n",
       "2  1.023323  1.012152  1.045098  1.024811  0.998982  1.003272  0.993698  \n",
       "3  0.996007  1.002119  0.993199  1.008782  1.011213  0.994391  1.010081  \n",
       "4  0.996492  0.991543  1.016057  0.996706  0.991532  0.997770  1.004185  \n",
       "\n",
       "[5 rows x 490 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prixFull = data.copy().iloc[1:data.shape[0],]\n",
    "for i in range(0, prixFull.shape[0]) :\n",
    "    prixFull.iloc[i, ] = data.iloc[i+1, ] / data.iloc[i, ]\n",
    "prixFull = prixFull.reset_index(drop = True)\n",
    "print(\"Dimensions du DF 'prixFull' : {}\".format(prixFull.shape))\n",
    "prixFull.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# justification de la reduction de dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABI</th>\n",
       "      <th>ABK</th>\n",
       "      <th>ABS</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACE</th>\n",
       "      <th>ACS</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.016782</td>\n",
       "      <td>1.004129</td>\n",
       "      <td>1.001867</td>\n",
       "      <td>0.991661</td>\n",
       "      <td>1.004042</td>\n",
       "      <td>1.014894</td>\n",
       "      <td>1.011933</td>\n",
       "      <td>1.006486</td>\n",
       "      <td>0.998745</td>\n",
       "      <td>0.995233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.990312</td>\n",
       "      <td>1.022907</td>\n",
       "      <td>0.985217</td>\n",
       "      <td>0.991956</td>\n",
       "      <td>0.988961</td>\n",
       "      <td>0.996646</td>\n",
       "      <td>1.020702</td>\n",
       "      <td>0.991586</td>\n",
       "      <td>0.988691</td>\n",
       "      <td>1.004508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.990217</td>\n",
       "      <td>0.995406</td>\n",
       "      <td>1.001009</td>\n",
       "      <td>1.001843</td>\n",
       "      <td>1.004465</td>\n",
       "      <td>1.009255</td>\n",
       "      <td>1.000770</td>\n",
       "      <td>1.011916</td>\n",
       "      <td>1.007262</td>\n",
       "      <td>1.001122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.983900</td>\n",
       "      <td>0.967840</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>1.011405</td>\n",
       "      <td>1.013727</td>\n",
       "      <td>1.017924</td>\n",
       "      <td>1.000257</td>\n",
       "      <td>1.004103</td>\n",
       "      <td>0.998558</td>\n",
       "      <td>0.998879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.992934</td>\n",
       "      <td>0.991208</td>\n",
       "      <td>0.999622</td>\n",
       "      <td>0.994907</td>\n",
       "      <td>0.985040</td>\n",
       "      <td>1.007371</td>\n",
       "      <td>1.008464</td>\n",
       "      <td>0.980455</td>\n",
       "      <td>1.006318</td>\n",
       "      <td>0.997756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AA      AAPL       ABC       ABI       ABK       ABS       ABT  \\\n",
       "0  1.016782  1.004129  1.001867  0.991661  1.004042  1.014894  1.011933   \n",
       "1  0.990312  1.022907  0.985217  0.991956  0.988961  0.996646  1.020702   \n",
       "2  0.990217  0.995406  1.001009  1.001843  1.004465  1.009255  1.000770   \n",
       "3  0.983900  0.967840  0.998614  1.011405  1.013727  1.017924  1.000257   \n",
       "4  0.992934  0.991208  0.999622  0.994907  0.985040  1.007371  1.008464   \n",
       "\n",
       "        ACE       ACS         A  \n",
       "0  1.006486  0.998745  0.995233  \n",
       "1  0.991586  0.988691  1.004508  \n",
       "2  1.011916  1.007262  1.001122  \n",
       "3  1.004103  0.998558  0.998879  \n",
       "4  0.980455  1.006318  0.997756  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Réduction de la dimension  \n",
    "prix = prixFull.iloc[:, 0:10]\n",
    "prix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(prix.shape[1])/prix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition d'une fonctionde perte\n",
    "\n",
    "A chaque étape $t$, on choisit un portefeuille d'actions $x_t \\in \\mathbb{R}^{490}$. La valeur des indices $a_t$ à chaque période est donnée par l'accroissement $c_t = \\frac{a_{t+1}}{a_t}$. L'investisseur subit une perte logarithmique $\\mathcal{L}(x_t) = -\\log(x_t^Tc_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit de normer l'ensemble des placements à $1$. Les décisions possibles appartiennent donc au simplexe $K = \\{x \\in \\mathbb{R}_+^{490} \\text{ tq } x^Tu = 1\\}$ où le vecteur $u$ n'est composé que de $1$. On définit la fonction de projection sur cet ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de la descente de gradient en ligne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descente de gradient dans le cadre ** en ligne**. \n",
    "\n",
    "Pas : $\\eta_1, ..., \\eta_T$ avec $\\eta_t=\\frac{1}{H(t+1)}$\n",
    "\n",
    ">  Etape 1: On choisit arbitrirment $x_1$\n",
    "\n",
    "> Etape 2: Pour tout $t>1$: on \n",
    ">> $x_{t+1} = \\Pi_K(x_t - \\eta_{t+1} \\nabla f_t (x_t))$\n",
    "\n",
    "\n",
    "\n",
    " >>avec $\\Pi_K(y)$: une projection sur $K$, \n",
    "$\\Pi_K(y) = \\underset{x \\in K}{\\text{argmin}} ||x-y||$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'cvxpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ccb5d004725a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mcvxpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcvx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprojectionSimplexe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'cvxpy'"
     ]
    }
   ],
   "source": [
    "import cvxpy as cvx\n",
    "import numpy as np\n",
    " \n",
    "def projectionSimplexe(y):\n",
    "    d = len(y)\n",
    "    u = np.ones(d)\n",
    "    x = cvx.Variable(d)\n",
    "    obj = cvx.Minimize(cvx.sum_squares(x - y))\n",
    "    constr = [x >= 0, u*x == 1]\n",
    "    prob = cvx.Problem(obj, constr)\n",
    "    prob.solve()\n",
    " \n",
    "    return np.array(x.value).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def Loss(x, c):\n",
    "    return -math.log((x * c).sum())\n",
    "\n",
    "def Gradient(x, c):\n",
    "    scalarProd = (x * c).sum()\n",
    "    return -c / scalarProd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'projectionSimplexe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4e2e552a207a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mogdResult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOnlineGradientDescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mprix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-4e2e552a207a>\u001b[0m in \u001b[0;36mOnlineGradientDescent\u001b[0;34m(df, init, H)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mOnlineGradientDescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx_t_normalise\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mprojectionSimplexe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnew_x_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_t_normalise\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprediction\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'projectionSimplexe' is not defined"
     ]
    }
   ],
   "source": [
    "def OnlineGradientDescent(df, init, H = 0.01):  \n",
    "    x_t_normalise= projectionSimplexe(init) \n",
    "    new_x_t=[x_t_normalise]\n",
    "    loss = []\n",
    "    prediction= []\n",
    "    for i in range(df.shape[0]-1):\n",
    "        c = df.iloc[i, ]\n",
    "        eta_t=1/(H*(i+1))\n",
    "        new_x_t += [projectionSimplexe((new_x_t[i] - eta_t * Gradient(new_x_t[i], c)).tolist())]\n",
    "        c = df.iloc[i+1, ]\n",
    "        loss += [Loss(new_x_t[i+1], c) ]\n",
    "    res = dict()\n",
    "    res[\"prediction\"] = new_x_t\n",
    "    res[\"loss\"] = loss\n",
    "    return(res)\n",
    "\n",
    "ogdResult = OnlineGradientDescent(prix, np.ones(prix.shape[1])/prix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ogdError = ogdResult[\"loss\"]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "pd.Series(ogdError).plot()\n",
    "plt.title(\"Perte logarithmique au cours du temps\")\n",
    "plt.subplot(1,2,2)\n",
    "pd.Series(np.cumsum(ogdError)).plot()\n",
    "plt.title(\"Perte cumulée au cours du temps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de Newton en ligne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descente de gradient dans le cadre ** en ligne**. \n",
    "\n",
    "\n",
    ">  Etape 1: On choisit arbitrirment $x_1$\n",
    "\n",
    "> Etape 2: Pour tout $t>1$: on a\n",
    "\n",
    ">> $$\\Delta_{t-1}=\\Delta f_{t-1}(x_{t-1})$$\n",
    "\n",
    ">> $$A_{t-1}= \\sum_{\\tau=1}^{t-1}\\Delta_{\\tau}\\Delta_{\\tau}^T$$\n",
    "\n",
    "\n",
    ">> $$b_{t-1}= \\sum_{\\tau=1}^{t-1}\\Delta_{\\tau}\\Delta_{\\tau}^T x_\\tau- \\frac{1}{\\beta}\\Delta_{\\tau}$$\n",
    "\n",
    "\n",
    "\n",
    ">> $$x_{t+1} = \\Pi_K^{A_{t-1}}(A_{t-1}b_{t-1})$$\n",
    "\n",
    "\n",
    "\n",
    " >>avec $\\Pi_K^{A_{t-1}}(y)$: une projection sur $K$, \n",
    "$$\\Pi_K^{A_{t-1}}(y) = \\underset{x \\in K}{\\text{argmin}} (x-y)^T A_{t-1}(x-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def projectionSelonMatrice(A, y) :\n",
    "    d = len(y)\n",
    "    u = np.ones(d)\n",
    "    x = cvx.Variable(d)\n",
    "    obj = cvx.Minimize(cvx.quad_form(x - y, A))\n",
    "    constr = [x >= 0, u*x == 1]\n",
    "    prob = cvx.Problem(obj, constr)\n",
    "    prob.solve()\n",
    " \n",
    "    return np.array(x.value).squeeze()\n",
    "\n",
    "def OnlineNewtonStep(df, init, beta = 0.01):\n",
    "    start = time.time()\n",
    "    print(\"--- ALGORITHME ONLINE NEWTON STEP\")\n",
    "    n, p = df.shape\n",
    "    portfolio = [ init ]\n",
    "    loss = []\n",
    "    A = np.zeros((p, p))\n",
    "    b = np.zeros(p)\n",
    "    print(\"--- Nombre d'itérations : {}\".format(n))\n",
    "    it10pct = n // 10\n",
    "    for i in range(n-1):\n",
    "        c = df.iloc[i, ]\n",
    "        delta = Gradient(portfolio[i], c).values.reshape(p, 1)\n",
    "        Anew = np.dot(delta, np.transpose(delta))\n",
    "        delta = np.transpose(delta)[0]\n",
    "        bNew = np.dot(Anew, portfolio[i]) - delta / beta\n",
    "        A += Anew\n",
    "        b += bNew\n",
    "        portfolio += [ projectionSelonMatrice(A, np.dot(np.linalg.pinv(A), b)) ]\n",
    "        c = df.iloc[i+1, ]\n",
    "        loss += [ Loss(portfolio[i+1], c) ]\n",
    "        if i % it10pct == 0:\n",
    "            print(\"----- {}% de l'algorithme\".format((i // it10pct) * 10))\n",
    "        \n",
    "    print(\" --- FIN DE L'ALGORITHME\")\n",
    "    print(\" --- Temps écoulé : {} secondes\".format(time.time() - start))\n",
    "    res = dict()\n",
    "    res[\"prediction\"] = portfolio\n",
    "    res[\"loss\"] = loss\n",
    "    return(res)\n",
    "\n",
    "onsResult = OnlineNewtonStep(prix, np.ones(prix.shape[1])/prix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onsError = onsResult[\"loss\"]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "pd.Series(onsError).plot()\n",
    "plt.title(\"Perte logarithmique au cours du temps\")\n",
    "plt.subplot(1,2,2)\n",
    "pd.Series(np.cumsum(onsError)).plot()\n",
    "plt.title(\"Perte cumulée au cours du temps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation en ligne avec poids exponentiels\n",
    "\n",
    "On commence par introduire une marche aléatoire permettant d'échantillonner selon $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Généralisation de l'algorithme $EWA$ au cas continu.\n",
    "\n",
    "\n",
    " >> Etape 1: Poids exponentiels $w_{t+1}(x) = e^{-\\alpha\\sum\\limits_{\\tau = 1}^{t} f_{\\tau}(x)} = \\prod\\limits_{\\tau = 1}^{t} h_{\\tau}(x)$\n",
    " \n",
    " \n",
    "\n",
    "\n",
    ">> Etape 2:Mise à jour $x_{t+1} = \\frac{\\int_K x w_{t+1}(x)dx}{\\int_K w_{t+1}(x)dx}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def computeWeight(alpha, df, t, x):\n",
    "    subDf = df.iloc[0:t,]\n",
    "    prod = np.dot(subDf, x)\n",
    "    # print(\"sous-DF : {} - x : {} - prod : {}\".format(subDf.shape, x.shape, prod.shape))\n",
    "    res = math.exp(-alpha * prod.sum())\n",
    "    return(res)\n",
    "\n",
    "def randomWalk(alpha, df, t, delta):\n",
    "    n = df.shape[1]\n",
    "    x = np.ones(n) / n\n",
    "    currentLkh = computeWeight(alpha, df, t, x)\n",
    "    nbIt = 10 * df.shape[1]\n",
    "    \n",
    "    for i in range(nbIt):\n",
    "        j = np.random.choice([k for k in range(n-1)])\n",
    "        X = np.random.choice([-1,1])\n",
    "        y = x.copy()\n",
    "        y[j] = y[j] + X*delta\n",
    "        y[n-1] = y[n-1] - X*delta\n",
    "        newLkh = computeWeight(alpha, df, t, y)\n",
    "        proba = min(1, currentLkh / newLkh)\n",
    "        if np.random.binomial(1, proba) == 1 :\n",
    "            x = y.copy()\n",
    "            currentLkh = newLkh\n",
    "    return x\n",
    "\n",
    "def portfolioExponentiallyWeighted(df, init, alpha = 0.01, delta = 0.01):\n",
    "    start = time.time()\n",
    "    print(\"--- ALGORITHME EXPONENTIALLY WEIGHTED\")\n",
    "    n = df.shape[0]\n",
    "    portfolio = [ init ]\n",
    "    loss = []\n",
    "    print(\"--- Nombre d'itérations : {}\".format(n))\n",
    "    it10pct = n // 10\n",
    "    for i in range(n-1):\n",
    "        c = df.iloc[i, ]\n",
    "        \n",
    "        portfolio += [ randomWalk(alpha, df, i, delta) ]\n",
    "        c = df.iloc[i+1, ]\n",
    "        loss += [ logLoss(portfolio[i+1], c) ]\n",
    "        if i % it10pct == 0:\n",
    "            print(\"----- {}% de l'algorithme\".format((i // it10pct) * 10))\n",
    "        \n",
    "    print(\" --- FIN DE L'ALGORITHME\")\n",
    "    print(\" --- Temps écoulé : {} secondes\".format(time.time() - start))\n",
    "    res = dict()\n",
    "    res[\"prediction\"] = portfolio\n",
    "    res[\"loss\"] = loss\n",
    "    return(res)\n",
    "    \n",
    "\n",
    "ewooResult = portfolioExponentiallyWeighted(prix, np.ones(prix.shape[1])/prix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ewooError = ewooResult[\"loss\"]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "pd.Series(ewooError).plot()\n",
    "plt.title(\"Perte logarithmique au cours du temps\")\n",
    "plt.subplot(1,2,2)\n",
    "pd.Series(np.cumsum(ewooError)).plot()\n",
    "plt.title(\"Perte cumulée au cours du temps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_rho(x,x_t,c, beta):\n",
    "    f_rho=  Loss(x_t,c)+ Gradient(x_t,c)*(x-x_t).sum() + beta/2* (Gradient(x_t,c)*(x-x_t).sum())^2\n",
    "    return  f_rho\n",
    "\n",
    "def f_rho_sum (x,prediction,df, beta):\n",
    "    som=0\n",
    "    for i in range(len(prediction)):\n",
    "        c = df.iloc[i, ]\n",
    "        som +=f_rho(x,prediction[i],c, beta)\n",
    "    return som"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def portfolioExponentiallyWeighted(df, init, beta = 0.01):\n",
    "    n, p = df.shape\n",
    "    prediction=[]\n",
    "    prediction=[init]\n",
    "    for i in range(n-1):\n",
    "        prediction += [minimize(f_rho_sum, x0, method='nelder-mead',options={'xtol': 1e-8, 'disp': True})]\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
